{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/deepstation/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/deepstation/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/deepstation/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/deepstation/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/deepstation/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/deepstation/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# 予め同じdirectoryに「helper.py」と「visualizations.py」をおいておく必要がある。\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model, Model\n",
    "import numpy as np\n",
    "from visualizations import GradCAM, GuidedGradCAM, GBP, LRP, CLRP, SGLRP, LRPA, LRPB, LRPE\n",
    "from helper import heatmap\n",
    "import innvestigate.utils as iutils\n",
    "import os\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import skimage as sk\n",
    "sk.__version__\n",
    "# limits tensorflow to a specific GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1_ (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 15,242,050\n",
      "Trainable params: 15,242,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # This will be your trained model instead.\n",
    "\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "# from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# imagesize = 224\n",
    "# basemodel = InceptionResNetV2(\n",
    "#     weights='imagenet',# imagenetでの学習セットを使用. kernel/filterは3x3\n",
    "#     input_shape=[imagesize, imagesize, 3], #サイズ二次元　+ RGB\n",
    "#     include_top=False, #全結合層を外す。imagenetの重みを凍結しない。更新あり。\n",
    "# )\n",
    "# layers = basemodel.output # 全結合層以降を構築\n",
    "# layers = GlobalAveragePooling2D()(layers)\n",
    "# layers = Dense(1024, activation='relu')(layers)\n",
    "# predictions = Dense(2, activation='softmax')(layers) # Dense(category-nos, activation関数) ここでは2クラス分類\n",
    "# model = Model(inputs=basemodel.input, outputs=predictions)\n",
    "\n",
    "model = load_model(\"./models/vgg16/Thres1_sgd0.001_epoch30_batch32.h5\") # 学習モデルを指定。\n",
    "# model.load_weights(\"models/tensorlog/inceptionV3/weights.36-0.96-0.84.hdf5\")\n",
    "\n",
    "model.layers[-2].name='dense_1_'\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1_ (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 15,242,050\n",
      "Trainable params: 15,242,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Only the partial model is needed for the visualizers. Use innvestigate.utils.keras.graph.pre_softmax_tensors()\n",
    "partial_model = Model(\n",
    "    inputs=model.inputs,\n",
    "    outputs=iutils.keras.graph.pre_softmax_tensors(model.outputs),\n",
    "    name=model.name,\n",
    ")\n",
    "partial_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Range of input images\n",
    "# keras_applications VGG16 weights assume a range of (-127.5, 127.5). Change this to a range suitable for your model.\n",
    "max_input = -127.5\n",
    "min_input = 127.5\n",
    "# max_input = 0\n",
    "# min_input = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:03<00:00, 24.04it/s]\n",
      "100%|██████████| 701/701 [00:23<00:00, 30.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792, 2)\n",
      "(792, 2)\n",
      "Confusion Matrix\n",
      "[[356 345]\n",
      " [  9  82]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "imagesize = 224\n",
    "width = height = imagesize\n",
    "\n",
    "def read_pil_image(img_path, height, width):\n",
    "        with open(img_path, 'rb') as f:\n",
    "            return np.array(Image.open(f).convert('RGB').resize((width, height)))\n",
    "\n",
    "prediction = []\n",
    "label = []\n",
    "datapath_posi = \"./test/posi/\"\n",
    "datapath_nega = \"./test/nega/\"\n",
    "files = os.listdir(datapath_posi)\n",
    "for p in tqdm(files[:]):\n",
    "    img = read_pil_image(datapath_posi + p, height, width)\n",
    "    img = img.reshape((1,imagesize,imagesize,3))\n",
    "    prediction.append(model.predict(img))\n",
    "    label.append([0,1])\n",
    "    \n",
    "files = os.listdir(datapath_nega)\n",
    "for p in tqdm(files[:]):\n",
    "    img = read_pil_image(datapath_nega + p, height, width)\n",
    "    img = img.reshape((1,imagesize,imagesize,3))\n",
    "    prediction.append(model.predict(img))\n",
    "    label.append([1,0])\n",
    "label = np.array(label).reshape((-1,2))\n",
    "prediction = np.array(prediction).reshape((-1,2))\n",
    "print(label.shape)\n",
    "print(prediction.shape)\n",
    "\n",
    "y_pred_onehot = prediction\n",
    "y_pred = np.argmax(y_pred_onehot, axis=1)\n",
    "y_pred_value = [y_pred_onehot[i][1] for i in range(y_pred.shape[0])]\n",
    "\n",
    "y_val_onehot = label\n",
    "y_val = np.argmax(y_val_onehot, axis=1)\n",
    "\n",
    "# y_val =validation_dataflow.classes\n",
    "# y_val_onehot = np_utils.to_categorical(y_val)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRPやLRP-Sequentialなどは一つずつ指定しないとmemoryエラーが発生する。他をコメントアウトして対応\n",
    "# Grad-CAM/Guided-GradCamなどは大丈夫。\n",
    "\n",
    "# Change this to load a list of images you want. For this example, we are only loading one image, but you can load a list of files.\n",
    "\n",
    "#os.system(\"ls ./test/posi/* > list-posi.txt\")\n",
    "list = open(\"list-posi.txt\", 'r') #名前だけのリスト。pathは含めない。\n",
    "#output = open(\"Prediction-negativetest.txt\", 'w')　# predictionの一覧を表示\n",
    "GGC = open(\"GGC-raw-data.txt\", \"w\")\n",
    "#output.write(\"[nega, posi]\\n\")\n",
    "\n",
    "singleimg = list.readline()\n",
    "\n",
    "while singleimg:\n",
    "    singleimg = singleimg.rstrip()\n",
    "    orig_imgs = [img_to_array(load_img(\"./test/posi/\"+singleimg, target_size=(imagesize, imagesize)))] #pathを指定。 \n",
    "\n",
    "    input_imgs = np.copy(orig_imgs)\n",
    "    example_id = 0\n",
    "    target_class = 1\n",
    "\n",
    "    # GradCAM and GuidedGradCAM requires a specific layer\n",
    "    target_layer = \"block3_conv3\" # VGG only　とりあえずblock3_conv3\n",
    "    predictions = model.predict(input_imgs)\n",
    "    pred_id = np.argmax(predictions[example_id])\n",
    "#    output.write(str(predictions)+\"\\n\")\n",
    "    \n",
    "    use_relu = False\n",
    "    \n",
    "    #Guided-GradCAMを./Guided-GradCAMにpngで保存\n",
    "    guidedgradcam_analyzer = GuidedGradCAM(\n",
    "        partial_model,\n",
    "        target_id=target_class,\n",
    "        layer_name=target_layer,\n",
    "        relu=False,\n",
    "        allow_lambda_layers = True\n",
    "\n",
    "    )\n",
    "    analysis_guidedgradcam = guidedgradcam_analyzer.analyze(input_imgs) \n",
    "    np.set_printoptions(threshold=1000000)\n",
    "    print(singleimg + \"\\n\" + str(analysis_guidedgradcam) + \"\\nend\\n\", file=GGC)    \n",
    "    \n",
    "    singleimg = list.readline()\n",
    "\n",
    "list.close()\n",
    "#output.close()\n",
    "GGC.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
